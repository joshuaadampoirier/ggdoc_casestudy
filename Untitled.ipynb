{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting fractures by applying neural networks to conventional well-logging data  \n",
    "Joshua Poirier  \n",
    "Geoscientist  \n",
    "NEOS  \n",
    "April 2017  \n",
    "\n",
    "### Abstract  \n",
    "\n",
    "This case study comes from Guangren Shi's chapter on Artificial Neural Networks from the book \"Data Mining and Knowledge Discovery for Geoscientists.\" You can\n",
    "purchase the book from [Amazon](https://www.amazon.com/Data-Mining-Knowledge-Discovery-Geoscientists/dp/0124104371/ref=sr_1_1?ie=UTF8&qid=1490908644&sr=8-1&keywords=data+mining+and+knowledge+discovery+for+geoscientists+%2B+guangren+shi) or directly from the publisher, [Elsevier](https://www.elsevier.com/books/data-mining-and-knowledge-discovery-for-geoscientists/shi/978-0-12-410437-2). The objective is to predict fractures using conventional well-logging data. This data has practical value when the data of the imaging log and core samples are limited.  \n",
    "\n",
    "Shi describes the scenario as follows:  \n",
    "\n",
    "> Located southeast of the Biyang Sag in Nanxiang Basin in central China, the Anpeng Oil-field covers an area of about 17.5 square kilometers, close to Tanghe-zaoyuan in the northwest-west, striking a large boundary fault in the south, and close to a deep sag in the east. As an inherited nose structure plunging from northwest to southeast, this oilfield is a simple structure without faults, where commercial oil and gas flows have been discovered (Ming et al., 2005; Wang et al., 2006). One of its favorable pool-forming conditions is that the fractures are found to be well developed at formations as deep as 2800 m or more. These fractures provide favorable oil-gas migration pathways and enlarged the accumulation space.\n",
    "\n",
    "Computationally, instead of writing the neural network code from scratch I'll be using TFLearn, a high level library built on top of TensorFlow to build neural networks. TensorFlow was developed by Google, and is open-source (free!).  \n",
    "\n",
    "### Introduction  \n",
    "\n",
    "The data was transcribed from Shi's book and includes data from 33 samples in Wells An1 and An2, of which he used 29 as learning samples; holding out 4 as a test set. The data features available are summarized below. Units are not given as each log has been normalized over the interval [0, 1].  \n",
    "\n",
    "| Variable name | Description                                                  |\n",
    "| ------------- | ------------------------------------------------------------ |\n",
    "| Sample        | Sample number                                                |\n",
    "| Well          | Well number                                                  |\n",
    "| Depth         | Measured depth in meters                                     |\n",
    "| DT            | Acoustic time                                                |\n",
    "| RHO           | Compensated neutron density                                  |\n",
    "| PHIN          | Compensated neutron porosity                                 |\n",
    "| R_XO          | Microspherically focused resistivity                         |\n",
    "| R_LLD         | Deep laterolog resistivity                                   |\n",
    "| R_LLS         | Shallow laterolog resistivity                                |\n",
    "| R_DS          | Absolute difference between R_LLD and R_LLS                  |\n",
    "| IL            | Fracture identification determined by imaging log (1=fracture, 2=nonfracture) |\n",
    "\n",
    "I'll get started by loading in the Python libraries I'll be using!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tflearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some data before we can get started with neural networks, let's load it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>Well</th>\n",
       "      <th>Depth</th>\n",
       "      <th>DT</th>\n",
       "      <th>RHO</th>\n",
       "      <th>PHIN</th>\n",
       "      <th>R_XO</th>\n",
       "      <th>R_LLD</th>\n",
       "      <th>R_LLS</th>\n",
       "      <th>R_DS</th>\n",
       "      <th>IL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>An1</td>\n",
       "      <td>3065.13</td>\n",
       "      <td>0.5557</td>\n",
       "      <td>0.2516</td>\n",
       "      <td>0.8795</td>\n",
       "      <td>0.3548</td>\n",
       "      <td>0.6857</td>\n",
       "      <td>0.6688</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>An1</td>\n",
       "      <td>3089.68</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.8999</td>\n",
       "      <td>0.6792</td>\n",
       "      <td>0.5421</td>\n",
       "      <td>0.4071</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>An1</td>\n",
       "      <td>3098.21</td>\n",
       "      <td>0.4444</td>\n",
       "      <td>0.1961</td>\n",
       "      <td>0.5211</td>\n",
       "      <td>0.7160</td>\n",
       "      <td>0.7304</td>\n",
       "      <td>0.6879</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>An1</td>\n",
       "      <td>3102.33</td>\n",
       "      <td>0.4028</td>\n",
       "      <td>0.3506</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.6218</td>\n",
       "      <td>0.6127</td>\n",
       "      <td>0.5840</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>An1</td>\n",
       "      <td>3173.25</td>\n",
       "      <td>0.3995</td>\n",
       "      <td>0.3853</td>\n",
       "      <td>0.0845</td>\n",
       "      <td>0.5074</td>\n",
       "      <td>0.8920</td>\n",
       "      <td>0.8410</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sample Well    Depth      DT     RHO    PHIN    R_XO   R_LLD   R_LLS  \\\n",
       "0       1  An1  3065.13  0.5557  0.2516  0.8795  0.3548  0.6857  0.6688   \n",
       "1       2  An1  3089.68  0.9908  0.0110  0.8999  0.6792  0.5421  0.4071   \n",
       "2       3  An1  3098.21  0.4444  0.1961  0.5211  0.7160  0.7304  0.6879   \n",
       "3       4  An1  3102.33  0.4028  0.3506  0.5875  0.6218  0.6127  0.5840   \n",
       "4       5  An1  3173.25  0.3995  0.3853  0.0845  0.5074  0.8920  0.8410   \n",
       "\n",
       "     R_DS  IL  \n",
       "0  0.0169   1  \n",
       "1  0.1350   1  \n",
       "2  0.0425   1  \n",
       "3  0.0287   1  \n",
       "4  0.0510   1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = 'data/fracture_data.csv'\n",
    "data = pd.read_csv(fname)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll take Shi's data splitting a step further by splitting the learning samples into training and validation subsets. This will help avoid overfitting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
